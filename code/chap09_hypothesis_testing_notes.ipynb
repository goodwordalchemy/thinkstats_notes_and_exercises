{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import thinkstats2, thinkplot\n",
    "import random, math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classical Hypothesis Testing** - Given a sample and an apparent effect, what is the probability of seeing such an effect by chance?\n",
    "\n",
    "1.  Choosing a **test statistic**.  e.g. difference in mean pregnancy length between two groups\n",
    "\n",
    "2.  Define **null hypothesis** - a model of the system based on the assumption that the apparent effect is not real.\n",
    "\n",
    "3.  Compute a **p-value**, the probability of seeing the apparent effict if the null hypothesis is true.\n",
    "\n",
    "4.  Interpret Stats.  If p-value is low, the effect is **statistically significant**, i.e. unlikely to have occurred by chance, and likely to appear in the larger population.  This doesn't necessarily mean that it is **significant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HypothesisTest(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.MakeModel()\n",
    "        self.actual = self.TestStatistic(data)\n",
    "        \n",
    "    def PValue(self, iters=1000):\n",
    "        self.test_stats = [self.TestStatistic(self.RunModel())\n",
    "                           for _ in range(iters)]\n",
    "        count = sum(1 for x in self.test_stats if x >= self.actual)\n",
    "        return float(count) / iters\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        raise UnimplementedMethodException()\n",
    "    \n",
    "    def MakeModel(self):\n",
    "        pass\n",
    "    \n",
    "    def RunModel(self):\n",
    "        raise UnimplementedMethodException()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CoinTest(HypothesisTest):\n",
    "    def TestStatistic(self, data):\n",
    "        heads, tails = data\n",
    "        test_stat = abs(heads - tails)\n",
    "        return test_stat\n",
    "    \n",
    "    def RunModel(self):\n",
    "        heads, tails = self.data\n",
    "        n = heads + tails\n",
    "        sample = [random.choice('HT') for _ in range(n)]\n",
    "        hist = thinkstats2.Hist(sample)\n",
    "        data = hist['H'], hist['T']\n",
    "        return data\n",
    "\n",
    "ct = CoinTest((140, 110))\n",
    "pvalue = ct.PValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by convention, a pvalue less than 5% is consitered significant.  But, p-value depends on the choice of test stats and the model of the null hypothesis, so p-values not precise.  less than 1% is unlikely to be due to chance.  Greater than 10% plausibly due to chance.  Between is grey area.\n",
    "\n",
    "##Testing a difference in means\n",
    "\n",
    "*  one way to model null hypothesis is by **permutation**, which is to take values for first babies and tohers and shuffle them, treating the two groups as one big group.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DiffMeansPermute(HypothesisTest):\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        group1, group2 = data\n",
    "        test_stat = abs(group1.mean() - group2.mean())\n",
    "        return test_stat\n",
    "    \n",
    "    def MakeModel(self):\n",
    "        group1, group2 = self.data\n",
    "        self.n, self.m = len(group1), len(group2)\n",
    "        self.pool = np.hstack((group1, group2))\n",
    "    \n",
    "    def RunModel(self):\n",
    "        np.random.shuffle(self.pool)\n",
    "        data = self.pool[:self.n], self.pool[self.n:]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nsfg.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.birthwgt_lb[df.birthwgt_lb > 20] = np.nan\n"
     ]
    }
   ],
   "source": [
    "import first\n",
    "\n",
    "live, firsts, others = first.MakeFrames()\n",
    "data = firsts.prglngth.values, others.prglngth.values\n",
    "ht = DiffMeansPermute(data)\n",
    "pvalue = ht.PValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.158"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this exists:\n",
    "\n",
    "```python\n",
    "ht.PlotCdf()\n",
    "thinkplot.Show(xlabel='test statistic',\n",
    "               ylabel='CDF')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsts_wgt = firsts.birthwgt_lb + (firsts.birthwgt_oz / 16)\n",
    "others_wgt = others.birthwgt_lb + (others.birthwgt_oz / 16)\n",
    "data2 = firsts_wgt, others_wgt\n",
    "ht2 = DiffMeansPermute(data2)\n",
    "pvalue2 = ht2.PValue()\n",
    "pvalue2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9961789525678455"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(firsts_wgt.mean() - others_wgt.mean()) * 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Other Test Statistics\n",
    "\n",
    "*  **one-sided** only takes one side of the distribution of differences.  p value is often 1/2 p-value for two sided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiffMeansOneSided(DiffMeansPermute):\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        group1, group2 = data\n",
    "        test_stat = group1.mean() - group2.mean()\n",
    "        return test_stat\n",
    "\n",
    "# rev_data = others.prglngth.values, firsts.prglngth.values\n",
    "OS_ht = DiffMeansOneSided(data)\n",
    "OS_pValue = OS_ht.PValue()\n",
    "OS_pValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.081"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiffStdPermute(DiffMeansPermute):\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        group1, group2 = data\n",
    "        test_stat = group1.std() - group2.std()\n",
    "        return test_stat\n",
    "\n",
    "STD_ht = DiffStdPermute(data)\n",
    "STD_pValue = STD_ht.PValue()\n",
    "STD_pValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Testing a correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CorrelationPermute(HypothesisTest):\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        xs, ys = data\n",
    "        test_stat = abs(thinkstats2.Corr(xs, ys))\n",
    "        return test_stat\n",
    "    \n",
    "    def RunModel(self):\n",
    "        xs, ys = self.data\n",
    "        xs = np.random.permutation(xs)\n",
    "        return xs, ys\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "live = live.dropna(subset=['agepreg', 'totalwgt_lb'])\n",
    "data = live.agepreg.values, live.totalwgt_lb.values\n",
    "ht = CorrelationPermute(data)\n",
    "pvalue = ht.PValue()\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Testing Proportions\n",
    "\n",
    "*  Total deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.133"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiceTest(HypothesisTest):\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        observed = data\n",
    "        n = sum(observed)\n",
    "        expected = np.ones(6) * n / 6.0\n",
    "        test_stat = sum(abs(observed - expected))\n",
    "        return test_stat\n",
    "    \n",
    "    def RunModel(self):\n",
    "        n = sum(self.data)\n",
    "        values = [1,2,3,4,5,6]\n",
    "        rolls = np.random.choice(values, n, replace=True)\n",
    "        hist = thinkstats2.Hist(rolls)\n",
    "        freqs = hist.Freqs(values)\n",
    "        return freqs\n",
    "\n",
    "dice_freqs = [8,9,19,5,8,11]\n",
    "dt = DiceTest(dice_freqs)\n",
    "pvalue = dt.PValue()\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question: *why is the \"expected\"*value for each die side not 10?  Instead it is the sum of the frequencies divided by 6?***\n",
    "\n",
    "*Because n is the total number of dice rolls.  expected is only ten if you roll the dice 60 times*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "*  **Chi-squared tests**\n",
    "$$\n",
    "\\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i}\n",
    "$$\n",
    "  where $O_i$ are the observed frequencies and $E_i$ are the expected frequencies.  Squaring the deviations (rather than taking absolute values) gives more weight to large deviations.  Dividing through expected standardizes the deviations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.033"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiceChiTest(DiceTest):\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        observed = data\n",
    "        n = sum(observed)\n",
    "        expected = np.ones(6) * n / 6\n",
    "        test_stat = sum((observed - expected)**2 / expected)\n",
    "        return test_stat\n",
    "\n",
    "chi_dt = DiceChiTest(dice_freqs)\n",
    "pvalue = chi_dt.PValue()\n",
    "pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PregLengthTest(HypothesisTest):\n",
    "    \n",
    "    def MakeModel(self):\n",
    "        firsts, others = self.data\n",
    "        self.n = len(firsts)\n",
    "        self.pool = np.hstack((first, others))\n",
    "        \n",
    "        pmf = thinkstats2.Pmf(self.pool)\n",
    "        self.values = range(35, 44)\n",
    "        self.expected_probs = np.array(pmf.Probs(self.values))\n",
    "    \n",
    "    ##Null hypothesis is that both samples are drawn from the same distribution\n",
    "    def RunModel(self):\n",
    "        np.random.shuffle(self.pool)\n",
    "        data = self.pool[:self.n], self.pool[self.n:]\n",
    "        return data\n",
    "    \n",
    "    def TestStatistic(self, data):\n",
    "        firsts, others = data\n",
    "        stat = self.ChiSquared(firsts) + self.ChiSquared(others)\n",
    "        return stat\n",
    "    \n",
    "    def ChiSquared(self, lengths):\n",
    "        hist = thinkstats2.Hist(lengths)\n",
    "        observed = np.array(hist.Freqs(self.values))\n",
    "        \n",
    "        ##expected_probs must be multiplied by length to get\n",
    "        ##actual values\n",
    "        expected = self.expected_probs * len(lengths)\n",
    "        stat = sum((observed - expected)**2 / expected)\n",
    "        return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = firsts.prglngth.values, others.prglngth.values\n",
    "ht = PregLengthTest(data)\n",
    "p_value = ht.PValue()\n",
    "p_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **False positive** rate for a p-value threshold of 5% is 1 out of 20, because p-value is the probability that a random value from the null hypothesis cdf reproduces an effect.  If p-value is 5%, then test probabilities that exceed the 95th percentile will show up positive.  This happens 5 percent of the time.\n",
    "\n",
    "**False negative rate** depends on actual effect size.  Can be computed by using observed samples as a model of the population and running hypothesis tests with simulated data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FalseNegRate(data, num_runs=100):\n",
    "    group1, group2 = data\n",
    "    count = 0.0\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        ##resample takes a sequence and draws a sample with \n",
    "        ##the same length with replacement\n",
    "        sample1 = thinkstats2.Resample(group1)\n",
    "        sample2 = thinkstats2.Resample(group2)\n",
    "        \n",
    "        ht = DiffMeansPermute((sample1, sample2))\n",
    "        pvalue = ht.PValue(iters=101)\n",
    "        if pvalue > 0.05:\n",
    "            count += 1\n",
    "    return count / num_runs\n",
    "\n",
    "neg_rate = FalseNegRate(data)\n",
    "neg_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**power of a test** - This means that, if the actual difference in mean pregnancy is 0.078 weeks, we expect an experiment with this sample size to yield a positive test only 30% of the time.  In general, if power of 80% is considered acceptable--anything less suggests the difference is too small to detect with this sample size.\n",
    "\n",
    "**partitioning** use one set for exploration and the other for testing.\n",
    "\n",
    "###Exercise 9.1\n",
    "\n",
    "*  difference in means\n",
    "*  correlation permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def SampleSize(df, size):\n",
    "    sampled_df = thinkstats2.SampleRows(df, size, replace=True)\n",
    "    return sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import estimation\n",
    "\n",
    "def testError(df1, df2, hTest, sampleFrac):\n",
    "    pvs = []\n",
    "    for i in range(25):\n",
    "        data1 = SampleSize(df1, len(df1) / sampleFrac)\n",
    "        data2 = SampleSize(df2, len(df2) / sampleFrac)\n",
    "        data = data1.values, data2.values\n",
    "        ht = hTest(data)\n",
    "        pValue = ht.PValue()\n",
    "        pvs.append(pValue)\n",
    "    \n",
    "    return reduce(lambda x, y: x + y, pvs) / len(pvs)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# live, firsts, others = first.MakeFrames()\n",
    "# data = firsts.prglngth.values, others.prglngth.values\n",
    "# ht = DiffMeansPermute(data)\n",
    "# pvalue = ht.PValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.12648\n",
      "45 0.04792\n",
      "40 0.05976\n",
      "35 0.02136\n",
      "30 0.01868\n",
      "25 0.0\n",
      "20 0.0\n",
      "15 0.0\n",
      "10 0.0\n",
      "5 0.0\n"
     ]
    }
   ],
   "source": [
    "means = []\n",
    "for i in np.arange(50, 1, -5):\n",
    "    mean = testError(data2[0], data2[1], DiffMeansPermute, i)\n",
    "    print i, mean\n",
    "    means.append(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first positive p-test I got was at sample size of 4418 / 25.  This experiment clearly showed that the power of the experiment--i.e. the rate that a positive is detected when it exists--greatly increases with increasing sample size.  Or at least I hypothesize that it does, and if I ran a correlation test, I think it would come up positive, and if I ran a p-test on that supposed correlation, I think it would also stand up to random variations.\n",
    "\n",
    "###Exercise 9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DiffMeansResample(DiffMeansPermute):\n",
    "    \n",
    "    def RunModel(self):\n",
    "        cdf = thinkstats2.Cdf(self.pool)\n",
    "        group1 = np.random.choice(self.pool, self.n, replace=True)\n",
    "        group2 = np.random.choice(self.pool, self.m, replace=True)\n",
    "        data = group1, group2\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight vs preglength 0.0\n",
      "first v others weight 0.0\n",
      "first v others, preglength 0.168\n"
     ]
    }
   ],
   "source": [
    "data = live.totalwgt_lb.values, live.prglngth.values\n",
    "ht = DiffMeansResample(data)\n",
    "pvalue = ht.PValue()\n",
    "print 'weight vs preglength',pvalue\n",
    "\n",
    "data = firsts.totalwgt_lb.values, others.totalwgt_lb.values\n",
    "ht = DiffMeansResample(data)\n",
    "pvalue = ht.PValue()\n",
    "print  'first v others weight', pvalue\n",
    "\n",
    "data = firsts.prglngth.values, others.prglngth.values\n",
    "ht = DiffMeansResample(data)\n",
    "pvalue = ht.PValue()\n",
    "print 'first v others, preglength', pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
